#!/usr/bin/env python3
from bs4 import BeautifulSoup, Comment
import json
import os
import pydantic_ai
from pydantic_ai import Agent
from pydantic import BaseModel,Field
from typing import Dict, List, Optional
import requests
from markdownify import markdownify as md
from googlesearch import search
from dotenv import load_dotenv
import time
import ListeStartup
import re
import listeLiens


load_dotenv()
api=os.getenv('GOOGLE_GEMINI_KEY')
gemini='gemini-1.5-flash'


"""Creation des classes demand√©es √† l'agent IA"""
class ResultatParLien(BaseModel):
    Nom: str = Field(..., description="Nom de la startup")
    Objectif: str = Field(..., description="Description d√©taill√©e en fran√ßais de la mission et de la valeur ajout√©e de la startup , il faut au moins 5 lignes claires et pr√©cises.")
    Probleme: str = Field(..., description="Probl√®me trait√© en fran√ßais, son importance et ses cons√©quences sur le march√©")
    Secteur: str = Field(..., description="Secteur d'activit√© de la startup")
class ResulatFinal(BaseModel):
    Resultat: List[ResultatParLien] = Field(...,description="Le resultat final qui est le ResultatParLien de chaque lien")
    
webAgent = Agent(
    gemini,
    result_type= ResulatFinal,
    system_prompt = (
    "Analyse lien par lien (il faut parcourir tous les liens dans la liste {listeLiens.listeLiens}}) dans la liste ournie par l'utilisateur. Pour chaque lien dans la liste qui correspondant √† une startup, "
    "utilise l'outil {webAgent} afin de comprendre son objectif, son domaine d'activit√© et sa mission, le tout en fran√ßais.","Tu traiteras tous les liens de la liste et imp√©rativement ." ,
    "Si tu ne comprend pas la premi√®re fois un lien tu devra reanalys√© le contenu qui te sera retourn√© par l'outil {visite_lien} jusqu'√† ce que tu comprennes."
)
)


@webAgent.tool_plain
def visiter_lien(lien: str):
    """Un lien est donn√© en arg le but est de scrappe les donn√©es du lien et en extraire le texte """
    response = requests.get(lien)
    soup= BeautifulSoup(response.text,'html.parser')
    for script_or_style in soup(['script', 'style']):
        script_or_style.decompose()
    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
        comment.extract()
    final= soup.prettify()
    return md(final)

def main():
    resultat= webAgent.run_sync(f"{listeLiens.listeLiens}").data
    res_dict=resultat.model_dump()
    with open("Description","a") as file:
        for i, dico in enumerate(res_dict['Resultat']):
            file.write(f"### üöÄ Startup {i}: {dico["Nom"]} \n\n")
            file.write(f"Objectif de la startup: \n {dico['Objectif']}\n\n")
            file.write(f"**üìå Probl√®me:** \n {dico['Probleme']}\n\n")
            file.write(f"**üè¢ Secteur:** \n {dico['Secteur']}\n\n")
            print(f" Stratup n¬∞{i} analys√©e")

if __name__ == "__main__":
    main()